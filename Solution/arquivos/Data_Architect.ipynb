{"cells":[{"cell_type":"markdown","source":["# iFood Data Architect Test - Marcio de Lima\n\nProcess semi-structured data and build a datalake that provides efficient storage and performance. The datalake must be organized in the following 2 layers:\n* raw layer: Datasets must have the same schema as the source, but support fast structured data reading\n* trusted layer: datamarts as required by the analysis team"],"metadata":{}},{"cell_type":"code","source":["#Utility functions\ndef getTags() -> dict: \n  return sc._jvm.scala.collection.JavaConversions.mapAsJavaMap(\n    dbutils.entry_point.getDbutils().notebook().getContext().tags()\n  )\n\ndef getTag(tagName: str, defaultValue: str = None) -> str:\n  values = getTags()[tagName]\n  try:\n    if len(values) > 0:\n      return values\n  except:\n    return defaultValue\n\ndef getUsername() -> str:\n  import uuid\n  try:\n    return getTag(\"user\", str(uuid.uuid1()).replace(\"-\", \"\"))\n  except:\n    return \"userMDL\"\n\n# Get the user's userhome\ndef getUserhome() -> str:\n  username = getUsername()\n  return \"dbfs:/user/{}\".format(username)\n\ndef getWorkingDir(caminho:str) -> str:\n  workingDir = \"{}/{}\".format(getUserhome(), caminho)\n  return workingDir.replace(\"__\", \"_\")\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":2},{"cell_type":"code","source":["# Ifood S3 connection provided in the Challenge\n\n#Chaves de Acesso e Bucket do S3\nawsAccessKey = \"ENTER HERE\"\nsecretKey = \"ENTER HERE\".replace(\"/\", \"%2F\")\nawsBucketName = \"ifood-data-architect-test-source\"\n\n#Montagem da Conexão na S3\nmountPoint = f\"/mnt/ifood-{getUsername()}\"\ntry:\n  dbutils.fs.unmount(mountPoint) \nexcept:\n  ()\n\ntry:\n  mountTarget = \"s3a://{}:{}@{}\".format(awsAccessKey, secretKey, awsBucketName)\n  dbutils.fs.mount(mountTarget, mountPoint)\nexcept:\n  ()\n\n\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">/mnt/ifood-marcio_de_lima@yahoo.com.br has been unmounted.\n</div>"]}}],"execution_count":3},{"cell_type":"code","source":["#%fs ls '/mnt/ifood-marcio_de_lima@yahoo.com.br'"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":4},{"cell_type":"code","source":["# List Files\nconsumer = \"dbfs:{}/{}\".format(mountPoint, \"consumer.csv.gz\")\nrestaurant = \"dbfs:{}/{}\".format(mountPoint, \"restaurant.csv.gz\")\norder = \"dbfs:{}/{}\".format(mountPoint, \"order.json.gz\")\nstatus = \"dbfs:{}/{}\".format(mountPoint, \"status.json.gz\")\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":5},{"cell_type":"code","source":["# Decision on the use of the file inconsistency method in the s3 load for further analysis.\n\nmyBadRecordsConsumer = getWorkingDir(\"badRecordsPath/consumer/\")\nmyBadRecordsRestaurant = getWorkingDir(\"badRecordsPath/restaurant/\")\nmyBadRecordsOrder = getWorkingDir(\"badRecordsPath/order/\")\nmyBadRecordsStatus = getWorkingDir(\"badRecordsPath/status/\")\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":6},{"cell_type":"code","source":["# Loading data to DataFrames.\n\nconsumerDF = (spark.read\n  .option(\"delimiter\", \",\")\n  .option(\"header\", True)\n  .option(\"timestampFormat\", \"mm/dd/yyyy hh:mm:ss a\")\n  .option(\"inferSchema\", True)\n  .option(\"badRecordsPath\", myBadRecordsConsumer)\n  .csv(consumer)\n)\n\nrestaurantDF = (spark.read\n  .option(\"delimiter\", \",\")\n  .option(\"header\", True)\n  .option(\"timestampFormat\", \"mm/dd/yyyy hh:mm:ss a\")\n  .option(\"inferSchema\", True)\n  .option(\"badRecordsPath\", myBadRecordsRestaurant)\n  .csv(restaurant)\n)\n\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":7},{"cell_type":"code","source":["#Checking import inconsistency\ntry:\n  path = \"{}/*/*/*\".format(myBadRecordsConsumer)\n  display(spark.read.csv(path))\nexcept:\n  print('Nao encontrado inconsistencia no arquivo')"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Nao encontrado inconsistencia no arquivo\n</div>"]}}],"execution_count":8},{"cell_type":"code","source":["try:\n  path = \"{}/*/*/*\".format(myBadRecordsRestaurant)\n  display(spark.read.csv(path))\nexcept:\n  print('Nao encontrado inconsistencia no arquivo')"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Nao encontrado inconsistencia no arquivo\n</div>"]}}],"execution_count":9},{"cell_type":"code","source":["#Creating Schema structure for the provided JSON files.\n#Decision made by applying Schema in Json to gain performance in reading and processing data.\n#verDF = spark.read.json(order)\n#verDF.printSchema()\n\nfrom pyspark.sql.types import StructType, StructField, IntegerType, StringType, LongType, \\\nDoubleType, BooleanType, TimestampType, DateType, ArrayType, MapType\n\nschemaJsonStatus = StructType([\n  StructField(\"created_at\", TimestampType(), True), \n  StructField(\"order_id\", StringType(), True), \n  StructField(\"status_id\", StringType(), True),\n  StructField(\"value\", StringType(), True) \n\n])\n\nschemaJsonOrder = StructType([\n  StructField(\"cpf\", StringType(), True), \n  StructField(\"customer_id\", StringType(), True),\n  StructField(\"customer_name\", StringType(), True), \n  StructField(\"delivery_address_city\", StringType(), True), \n  StructField(\"delivery_address_country\", StringType(), True), \n  StructField(\"delivery_address_district\", StringType(), True),\n  StructField(\"delivery_address_external_id\", StringType(), True), \n  StructField(\"delivery_address_latitude\", StringType(), True) ,\n  StructField(\"delivery_address_longitude\", StringType(), True) ,\n  StructField(\"delivery_address_state\", StringType(), True) ,\n  StructField(\"delivery_address_zip_code\", StringType(), True) , \n  StructField(\"merchant_id\", StringType(), True) ,\n  StructField(\"merchant_latitude\", StringType(), True), \n  StructField(\"merchant_longitude\", StringType(), True), \n  StructField(\"merchant_timezone\", StringType(), True),\n  StructField(\"order_created_at\", TimestampType(), True),\n  StructField(\"order_id\", StringType(), True),\n  StructField(\"order_scheduled\", BooleanType(), True),\n  StructField(\"order_scheduled_date\", TimestampType(), True),\n  StructField(\"order_total_amount\", DoubleType(), True),\n  StructField(\"origin_platform\", StringType(), True), \n  StructField(\"items\", StringType(), True)\n])\n\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":10},{"cell_type":"code","source":["# Loading JSON data\norderDF = (spark.read\n  .schema(schemaJsonOrder)\n  .option(\"badRecordsPath\", myBadRecordsOrder)\n  .json(order)\n)\n\nstatusDF = (spark.read\n  .schema(schemaJsonStatus)\n  .option(\"badRecordsPath\", myBadRecordsStatus)\n  .json(status)\n)\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":11},{"cell_type":"code","source":["#Checking import inconsistency\ntry:\n  path = \"{}/*/*/*\".format(myBadRecordsStatus)\n  display(spark.read.json(path))\nexcept:\n  print('Nao encontrado inconsistencia no arquivo')"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Nao encontrado inconsistencia no arquivo\n</div>"]}}],"execution_count":12},{"cell_type":"code","source":["#Checking import inconsistency\ntry:\n  path = \"{}/*/*/*\".format(myBadRecordsOrder)\n  display(spark.read.json(path))\nexcept:\n  print('Nao encontrado inconsistencia no arquivo')"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Nao encontrado inconsistencia no arquivo\n</div>"]}}],"execution_count":13},{"cell_type":"code","source":["# Treatment of duplicate data in DataFrames if they exist.\nstatusDF = statusDF.dropDuplicates([\"status_id\"])\norderDF = orderDF.dropDuplicates([\"order_id\"])"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":14},{"cell_type":"code","source":["# Close connection S3\ntry:\n  dbutils.fs.unmount(mountPoint) \nexcept:\n  ()"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"markdown","source":["## Decision to use DeltaLake\n\n\nReasons and criteria below:\n\n* <b>ACID transactions</b> - Multiple writers can simultaneously modify a data set and see consistent views.\n* <b>DELETES/UPDATES/UPSERTS</b> - Writers can modify a data set without interfering with jobs reading the data set.\n* <b>Automatic file management</b> - Data access speeds up by organizing data into large files that can be read efficiently.\n* <b>Statistics and data skipping</b> - Reads are 10-100x faster when statistics are tracked about the data in each file, allowing Delta to avoid reading irrelevant information."],"metadata":{}},{"cell_type":"code","source":["# Work Directory's\ndeltaPathConsumer = getWorkingDir(\"consumer-data-delta/\")\ndeltaPathStatus = getWorkingDir(\"status-data-delta/\")\ndeltaPathOrder = getWorkingDir(\"order-data-delta/\")\ndeltaPathRestaurant = getWorkingDir(\"restaurant-data-delta/\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":17},{"cell_type":"code","source":["# Security Treatment - Cleaning of Table files, if any\n# These instructions are only for the challenge, in a PRD environment, the RAW tables would be incremented via batch or Stream in real time. \nspark.conf.set(\"spark.databricks.delta.retentionDurationCheck.enabled\", False)\ntry:\n  spark.sql(\"\"\" VACUUM delta.`{}` RETAIN 0 HOURS \"\"\".format(deltaPathConsumer))\nexcept Exception as err: \n  print(str(err).replace(\"\\\\n\", \"\\n\").replace(\"'\", \"\"))\n\ntry:\n  spark.sql(\"\"\" VACUUM delta.`{}` RETAIN 0 HOURS \"\"\".format(deltaPathStatus))\nexcept Exception as err: \n  print(str(err).replace(\"\\\\n\", \"\\n\").replace(\"'\", \"\"))\n\ntry:\n  spark.sql(\"\"\" VACUUM delta.`{}` RETAIN 0 HOURS \"\"\".format(deltaPathOrder))\nexcept Exception as err: \n  print(str(err).replace(\"\\\\n\", \"\\n\").replace(\"'\", \"\"))\n\ntry:\n  spark.sql(\"\"\" VACUUM delta.`{}` RETAIN 0 HOURS \"\"\".format(deltaPathRestaurant))\nexcept Exception as err: \n  print(str(err).replace(\"\\\\n\", \"\\n\").replace(\"'\", \"\"))\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":18},{"cell_type":"code","source":["# Write Tables\n# RAW Tables as requested in the Challenge.\n# The OVERWRITE mode was placed, as the origin of the S3 files was not informed, if they are generated day by day only with updates for example and not as a FULL load, we can exchange OVERWRITE for the append or work with the concept of MERGE in DeltaLake . Thus, the most simple and practical method was chosen for the challenge. \n\nspark.conf.set(\"spark.databricks.delta.retentionDurationCheck.enabled\", True)\n\n# Write\n(statusDF.write\n  .mode(\"overwrite\")\n  .format(\"delta\")\n  .partitionBy(\"value\")\n  .save(deltaPathStatus) )\n\n\n(restaurantDF.write\n  .mode(\"overwrite\")\n  .format(\"delta\")\n  .partitionBy(\"merchant_state\")\n  .save(deltaPathRestaurant) )\n\n\n(consumerDF.write\n  .mode(\"overwrite\")\n  .format(\"delta\")\n  .save(deltaPathConsumer) )\n\n(orderDF.write\n  .mode(\"overwrite\")\n  .format(\"delta\")\n  .save(deltaPathOrder) )\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":19},{"cell_type":"code","source":["# Checking Order Table processing and writes\nsqlCmd = \"SELECT count(*) FROM delta.`{}` \".format(deltaPathOrder)\nprint(spark.sql(sqlCmd).first()[0])"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">2441075\n</div>"]}}],"execution_count":20},{"cell_type":"code","source":["# Checking Order Status processing and writes\nsqlCmd1 = \"SELECT count(*) FROM delta.`{}` \".format(deltaPathStatus)\nprint(spark.sql(sqlCmd1).first()[0])"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">7340326\n</div>"]}}],"execution_count":21},{"cell_type":"code","source":["# Optimizing RAW Tables\n# OPTIMIZE => Solving the Small Files Problem, Compression in 1GB.\n# Benefit: Gain in performance and decreased latency time.\n# ZORDER => Performance gains in queries\n\nspark.sql(\"\"\"OPTIMIZE delta.`{}` ZORDER by (order_id)\"\"\".format(deltaPathStatus))\n\nspark.sql(\"\"\"OPTIMIZE delta.`{}` ZORDER by (id)\"\"\".format(deltaPathRestaurant))\n\nspark.sql(\"\"\"OPTIMIZE delta.`{}` ZORDER by (customer_id)\"\"\".format(deltaPathConsumer))\n\nspark.sql(\"\"\"OPTIMIZE delta.`{}` ZORDER by (order_id)\"\"\".format(deltaPathOrder))\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[111]: DataFrame[path: string, metrics: struct&lt;numFilesAdded:bigint,numFilesRemoved:bigint,filesAdded:struct&lt;min:bigint,max:bigint,avg:double,totalFiles:bigint,totalSize:bigint&gt;,filesRemoved:struct&lt;min:bigint,max:bigint,avg:double,totalFiles:bigint,totalSize:bigint&gt;,partitionsOptimized:bigint,zOrderStats:struct&lt;strategyName:string,inputCubeFiles:struct&lt;num:bigint,size:bigint&gt;,inputOtherFiles:struct&lt;num:bigint,size:bigint&gt;,inputNumCubes:bigint,mergedFiles:struct&lt;num:bigint,size:bigint&gt;,mergedNumCubes:bigint&gt;,numBatches:bigint&gt;]</div>"]}}],"execution_count":22},{"cell_type":"markdown","source":["## Constructing Trust Layer \n\n\n  * Order dataset -  one line per order with all data from order, consumer, restaurant and the LAST status from order statuses dataset. To help analysis, it would be a nice to have: data partitioned on the restaurant LOCAL date.\n  * Order Items dataset - easy to read dataset with one-to-many relationship with Order dataset. Must contain all data from _order_ items column.\n  * Order statuses - Dataset containing one line per order with the timestamp for each registered event: CONCLUDED, REGISTERED, CANCELLED, PLACED.\n  * For the trusted layer, anonymize any sensitive data."],"metadata":{}},{"cell_type":"code","source":["# Work Directory\ndeltaPathOrderTrust = getWorkingDir(\"order-trust-delta/\")\ndeltaPathOrderItem = getWorkingDir(\"order-item-trust-delta/\")\ndeltaPathOrderStatus = getWorkingDir(\"order-status-trust-delta/\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":24},{"cell_type":"code","source":["# Creating the Status Table - Trust Layer\nustatusDF = spark.sql(\"SELECT order_id, created_at, value FROM delta.`{}` group by order_id, created_at, value\".format(deltaPathStatus))\n\n(ustatusDF\n  .write\n  .mode(\"overwrite\")\n  .format(\"delta\")\n  .partitionBy(\"value\")\n  .save(deltaPathOrderStatus) \n)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":25},{"cell_type":"code","source":["from pyspark.sql.functions import to_json, from_json\nfrom pyspark.sql.functions import explode\n\n#Schema Items\nschemaItem = ArrayType(StructType([\n  StructField(\"name\", StringType(), True), \n  StructField(\"quantity\", DoubleType(), True),\n  StructField(\"externalId\", StringType(), True),\n  StructField(\"sequence\", IntegerType(), True),\n  StructField(\"addition\", StructType([ StructField(\"value\", StringType(), True) , StructField(\"currency\", StringType(), True) ]), True),\n  StructField(\"discount\", StructType([ StructField(\"value\", StringType(), True) , StructField(\"currency\", StringType(), True) ]), True),\n  StructField(\"unitPrice\", StructType([ StructField(\"value\", StringType(), True) , StructField(\"currency\", StringType(), True) ]), True),\n  StructField(\"totalValue\", StructType([ StructField(\"value\", StringType(), True) , StructField(\"currency\", StringType(), True) ]), True),\n  StructField(\"customerNote\", StringType(), True),\n  StructField(\"integrationId\", StringType(), True),\n  StructField(\"categoryName\", StringType(), True),\n  StructField(\"totalAddition\", StructType([ StructField(\"value\", StringType(), True) , StructField(\"currency\", StringType(), True) ]), True),\n  StructField(\"totalDiscount\", StructType([ StructField(\"value\", StringType(), True) , StructField(\"currency\", StringType(), True) ]), True),\n  StructField(\"garnishItems\", ArrayType(StructType([ \n        StructField(\"name\", StringType(), True) , \n        StructField(\"quantity\", DoubleType(), True),\n        StructField(\"addition\", StructType([ StructField(\"value\", StringType(), True) , StructField(\"currency\", StringType(), True) ]), True),\n        StructField(\"discount\", StructType([ StructField(\"value\", StringType(), True) , StructField(\"currency\", StringType(), True) ]), True),\n        StructField(\"sequence\", IntegerType(), True),\n        StructField(\"unitPrice\", StructType([ StructField(\"value\", StringType(), True) , StructField(\"currency\", StringType(), True) ]), True),\n        StructField(\"categoryId\", StringType(), True),\n        StructField(\"categoryName\", StringType(), True),\n        StructField(\"integrationId\", StringType(), True),\n        StructField(\"totalValue\", StructType([ StructField(\"value\", StringType(), True) , StructField(\"currency\", StringType(), True) ]), True),\n              ])), True)\n  \n\n]))\n\n## Creation of the Trust Items Table - Query, Transformations and Treatments\nitemDFTrust = spark.sql(\"SELECT * FROM delta.`{}`\".format(deltaPathOrder)) \\\n            .select(col(\"order_id\"), from_json(\"items\", schemaItem).alias(\"items\")).cache() \\\n            .select(col(\"order_id\"), explode(\"items\").alias(\"e\")).selectExpr(\"order_id\", \\\n                                 \"e.name\", \\\n                                 \"e.quantity\", \\\n                                 \"e.externalId\", \\\n                                 \"e.sequence\", \\\n               \"(case when e.unitPrice.value = '0' then 0.00 else cast(e.unitPrice.value as long) / 100 end) as unitPrice_value\", \\\n                                 \"e.unitPrice.currency as unitPrice_currency\", \\\n               \"(case when e.totalValue.value = '0' then 0.00 else cast(e.totalValue.value as long) / 100 end) as totalValue_value\", \\\n                                \"e.totalValue.currency as totalValue_currency\", \\\n               \"(case when e.addition.value = '0' then 0.00 else cast(e.addition.value as long) / 100 end) as addition_value\", \\\n                                 \"e.addition.currency as addition_currency\", \\\n               \"(case when e.discount.value = '0' then 0.00 else cast(e.discount.value as long) / 100 end) as discount_value\", \\\n                                 \"e.discount.currency as discount_currency\", \\\n                                 \"e.customerNote\", \\\n                                 \"e.integrationId\", \\\n                                 \"e.categoryName\", \\\n               \"(case when e.totalAddition.value = '0' then 0.00 else cast(e.totalAddition.value as long) / 100 end) as totalAddition_value\", \\\n                                 \"e.totalAddition.currency as totalAddition_currency\", \\\n               \"(case when e.totalDiscount.value = '0' then 0.00 else cast(e.totalDiscount.value as long) / 100 end) as totalDiscount_value\", \\\n                                 \"e.totalDiscount.currency as totalDiscount_currency\", \\\n                                 \"e.garnishItems\"\n                                ).fillna(\"\")\n\n# Write Table Item in DeltaLake\n(itemDFTrust\n  .write\n  .mode(\"overwrite\")\n  .format(\"delta\")\n  .save(deltaPathOrderItem) \n)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":26},{"cell_type":"code","source":["# Creating the Order data with the requested Inner Joins and Data Anomimation\n\n# Last Order House Status\nultimoStatusDF = spark.sql(\"SELECT order_id, max(created_at) as created_at_status, max(value) as lastStatus FROM delta.`{}` group by order_id\".format(deltaPathOrderStatus)).drop(\"created_at_status\")\n\n# Generating Order with Status\norderDeltaDF = spark.sql(\"SELECT * FROM delta.`{}` \".format(deltaPathOrder)).join(ultimoStatusDF, \"order_id\") \\\n                .drop(\"cpf\").drop(\"delivery_address_latitude\").drop(\"delivery_address_longitude\") \\\n                .drop(\"created_at\").drop(\"status_id\").drop(\"value\").drop(\"items\")\n\n# Generating Order with Consumer\nconsumerDeltaDF = spark.sql(\"SELECT customer_id, created_at as customer_created_at, language as customer_language, active as customer_active FROM delta.`{}` \".format(deltaPathConsumer))\norderConsumerTrustDF = orderDeltaDF.join(consumerDeltaDF, \"customer_id\")\n\n# Generating Order with Restaurant\nrestaurantDeltaDF = spark.sql(\"SELECT id, to_date(created_at, 'yyyy-MM-dd') as merchant_created_at, enabled as merchant_enabled, \\\nprice_range as merchant_price_range, average_ticket as merchant_average_ticket, takeout_time as merchant_takeout_time, \\\ndelivery_time as merchant_delivery_time, minimum_order_value as merchant_minimum_order_value, merchant_zip_code, \\\nmerchant_city, merchant_state, merchant_country FROM delta.`{}`\".format(deltaPathRestaurant)).withColumnRenamed(\"id\",\"merchant_id\") \n\norderTrustDF = orderConsumerTrustDF.join(restaurantDeltaDF, \"merchant_id\").fillna(\"\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":27},{"cell_type":"code","source":["# Generating Order Data on DeltaLake\n(orderTrustDF\n  .write\n  .mode(\"overwrite\")\n  .format(\"delta\")\n  .save(deltaPathOrderTrust) \n)\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":28},{"cell_type":"code","source":["# Showing the table saved in DeltaLake - Order\ndisplay(spark.sql(\"SELECT * FROM delta.`{}` where order_id = '002bbaff-af82-4a50-9083-07f7967ca4ea'\".format(deltaPathOrderTrust)))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>merchant_id</th><th>customer_id</th><th>order_id</th><th>customer_name</th><th>delivery_address_city</th><th>delivery_address_country</th><th>delivery_address_district</th><th>delivery_address_external_id</th><th>delivery_address_state</th><th>delivery_address_zip_code</th><th>merchant_latitude</th><th>merchant_longitude</th><th>merchant_timezone</th><th>order_created_at</th><th>order_scheduled</th><th>order_scheduled_date</th><th>order_total_amount</th><th>origin_platform</th><th>lastStatus</th><th>customer_created_at</th><th>customer_language</th><th>customer_active</th><th>merchant_created_at</th><th>merchant_enabled</th><th>merchant_price_range</th><th>merchant_average_ticket</th><th>merchant_takeout_time</th><th>merchant_delivery_time</th><th>merchant_minimum_order_value</th><th>merchant_zip_code</th><th>merchant_city</th><th>merchant_state</th><th>merchant_country</th></tr></thead><tbody><tr><td>83cc6d5d-d91b-4316-88ac-acbd9cf7ab8c</td><td>58563e65-ce21-4c6a-9d81-91582e9416d5</td><td>002bbaff-af82-4a50-9083-07f7967ca4ea</td><td>FILIPE</td><td>RIO DE JANEIRO</td><td>BR</td><td>COPACABANA</td><td>2671315</td><td>RJ</td><td>22070</td><td>-43.19</td><td>-22.98</td><td>America/Sao_Paulo</td><td>2019-01-31T23:35:11.000+0000</td><td>false</td><td>null</td><td>61.1</td><td>IOS</td><td>REGISTERED</td><td>2018-01-07T20:47:54.420+0000</td><td>pt-br</td><td>true</td><td>2017-01-20</td><td>true</td><td>3</td><td>80.0</td><td>0</td><td>60</td><td>0.0</td><td>22071</td><td>RIO DE JANEIRO</td><td>RJ</td><td>BR</td></tr></tbody></table></div>"]}}],"execution_count":29},{"cell_type":"code","source":["# Showing the table saved in DeltaLake - OrderItem\ndisplay(spark.sql(\"SELECT * FROM delta.`{}` where order_id = '002bbaff-af82-4a50-9083-07f7967ca4ea'\".format(deltaPathOrderItem)))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>order_id</th><th>name</th><th>quantity</th><th>externalId</th><th>sequence</th><th>unitPrice_value</th><th>unitPrice_currency</th><th>totalValue_value</th><th>totalValue_currency</th><th>addition_value</th><th>addition_currency</th><th>discount_value</th><th>discount_currency</th><th>customerNote</th><th>integrationId</th><th>categoryName</th><th>totalAddition_value</th><th>totalAddition_currency</th><th>totalDiscount_value</th><th>totalDiscount_currency</th><th>garnishItems</th></tr></thead><tbody><tr><td>002bbaff-af82-4a50-9083-07f7967ca4ea</td><td>COCA COLA 2 LT</td><td>1.0</td><td>545925f105fa42ada820065b258efde7</td><td>4</td><td>8.2</td><td>BRL</td><td>8.2</td><td>BRL</td><td>0.0</td><td>BRL</td><td>0.0</td><td>BRL</td><td></td><td></td><td></td><td>0.0</td><td>BRL</td><td>0.0</td><td>BRL</td><td>List()</td></tr><tr><td>002bbaff-af82-4a50-9083-07f7967ca4ea</td><td>GIGANTE</td><td>1.0</td><td>98063bda8b574f2f9c5ba6de18c94ec9</td><td>1</td><td>0.0</td><td>BRL</td><td>0.0</td><td>BRL</td><td>0.0</td><td>BRL</td><td>0.0</td><td>BRL</td><td></td><td></td><td></td><td>0.0</td><td>BRL</td><td>0.0</td><td>BRL</td><td>List(List(MASSA TRADICIONAL , 1.0, List(0, BRL), List(0, BRL), 2, List(0, BRL), 0025, Escolha a sua Preferência, null, List(0, BRL)), List(CALABRESA ESPECIAL, 1.0, List(0, BRL), List(0, BRL), 3, List(5290, BRL), SBR, Escolha um sabor, null, List(5290, BRL)))</td></tr></tbody></table></div>"]}}],"execution_count":30},{"cell_type":"code","source":["# Showing the table saved in DeltaLake - Status\ndisplay(spark.sql(\"SELECT * FROM delta.`{}` where order_id = '002bbaff-af82-4a50-9083-07f7967ca4ea'\".format(deltaPathOrderStatus)))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>order_id</th><th>created_at</th><th>value</th></tr></thead><tbody><tr><td>002bbaff-af82-4a50-9083-07f7967ca4ea</td><td>2019-01-31T23:35:12.000+0000</td><td>PLACED</td></tr><tr><td>002bbaff-af82-4a50-9083-07f7967ca4ea</td><td>2019-01-31T23:35:11.000+0000</td><td>REGISTERED</td></tr><tr><td>002bbaff-af82-4a50-9083-07f7967ca4ea</td><td>2019-01-02T01:40:08.000+0000</td><td>CONCLUDED</td></tr></tbody></table></div>"]}}],"execution_count":31},{"cell_type":"markdown","source":["## Challenge Finished\n\n### Thanks\n\n### Marcio de Lima"],"metadata":{}}],"metadata":{"name":"Data_Architect","notebookId":611257038923130},"nbformat":4,"nbformat_minor":0}
